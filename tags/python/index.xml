<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on Sebastian Brunner</title>
    <link>https://sebastian-brunner.github.io/tags/python/</link>
    <description>Recent content in Python on Sebastian Brunner</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 28 Apr 2019 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://sebastian-brunner.github.io/tags/python/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Robot Simulation</title>
      <link>https://sebastian-brunner.github.io/project/robot_simulation/</link>
      <pubDate>Sun, 28 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://sebastian-brunner.github.io/project/robot_simulation/</guid>
      <description>&lt;p&gt;Our &lt;a href=&#34;https://www.dlr.de/rm/en/desktopdefault.aspx/tabid-11409&#34;&gt;AIMM&lt;/a&gt; robot was simulated using the Gazebo Framework and the ROS middleware. The robot&amp;rsquo;s 12 DOFs were simulated using Gazebo&amp;rsquo;s controller suite. On top of that, path planning, object detection, navigation, world model, and task control (RAFCON) modules were integrated. The simulation was used for industrial and domestic service scenarios (among others for the &lt;a href=&#34;https://ease-crc.org/&#34;&gt;EASE&lt;/a&gt; project).&lt;/p&gt;
&lt;p&gt;My responsibilities: Full Stack Component Integration, Controller Configuration, Autonomous Task Control, Task Execution Logging and Profiling, Semantic World Modeling&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RAFCON</title>
      <link>https://sebastian-brunner.github.io/project/rafcon/</link>
      <pubDate>Tue, 10 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://sebastian-brunner.github.io/project/rafcon/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Industrial Mobile Manipulation</title>
      <link>https://sebastian-brunner.github.io/project/industrial_mobile_robotics/</link>
      <pubDate>Mon, 02 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://sebastian-brunner.github.io/project/industrial_mobile_robotics/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://www.dlr.de/rm/en/desktopdefault.aspx/tabid-11409&#34;&gt;AIMM&lt;/a&gt; robot is an industrial mobile manipulator able to autonomously perceive and manipulate its environment. It was used for various scenarios (especially pre-assembly and logistics) in multiple projects and demos: &lt;a href=&#34;https://factory-of-the-future.dlr.de/&#34;&gt;FoF&lt;/a&gt;, &lt;a href=&#34;http://www.euroc-project.eu/&#34;&gt;EuRoC&lt;/a&gt;, &lt;a href=&#34;https://messe-muenchen.de/en/technical/events/automatica-2018.php&#34;&gt;Automatica 2018&lt;/a&gt;, &lt;a href=&#34;http://www.tapas-project.eu/&#34;&gt;TAPAS&lt;/a&gt; and &lt;a href=&#34;https://messe-muenchen.de/en/technical/events/automatica-2016.php&#34;&gt;Automatica 2016&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I am a core developer of AIMM&amp;rsquo;s software stack. My main responsibilities are creating software for autonomous task control and belief state modeling. We put a high effort in the system architecture of AIMM, as it is a complex, mobile robot with 13 DOFs (including an impedance controlled LWR 4+), four cameras systems (able to produce depth information using &lt;a href=&#34;https://core.ac.uk/download/pdf/11134866.pdf&#34;&gt;SGM&lt;/a&gt;) and laser scanners. It features six computers: One for low-level realtime control, one for navigation, one for object detection and scene reconstruction, one for path planning, one for data logging and processing and one for high level autonomy. On top of the multitude of software models we employ four different middlewares: &amp;lsquo;links and nodes&amp;rsquo; for realtime control, &amp;lsquo;sensornet&amp;rsquo; for high bandwidth sensor data, &amp;lsquo;ROS&amp;rsquo; for third party library integration and data visualization, and Kuka&amp;rsquo;s Sunrise middleware. We use continuous integration to develop and release new software. Furthermore, we employ a powerful release and dependency management toolchain to track and deploy consistent software versions to AIMM.&lt;/p&gt;
&lt;p&gt;My responsibilities: Autonomous Task Control, System Architecture, Task Execution Logging and Profiling, Semantic World Modeling&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Robex</title>
      <link>https://sebastian-brunner.github.io/project/robex/</link>
      <pubDate>Mon, 26 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://sebastian-brunner.github.io/project/robex/</guid>
      <description>&lt;p&gt;For the Demonstration Mission Space a dedicated scenario was chosen by the ROBEX lunar scientists. The scenario describes the installation of an active seismic network (ASN) on the Moonâ€™s surface. The main focus is the measurement of the internal structure and the composition of the upper layer, the lunar regolith. Other questions are the existence and composition of a central core of the Moon and if there is any seismic activity. The seismometers are planned to be transported by a rover and put down on surface by means of a robotic arm.&lt;/p&gt;
&lt;p&gt;The analogue mission on the Mt. Etna basically consisted of two individual experiments: First, our rover mapped the lander site, deployed the seismic instrument, waited until one measurement cycle was done, took it up again and repeated the measurement on several points of interest. In the second experiment, the rover had to set up a seismic network consisting of four instruments that had to be arranged at three corner points and the center point of an equilateral triangle of about 100 m. Both scenarios required the robot to be equipped with highly robust navigation, perception and manipulation capabilities in order to place the instruments carefully and precisely onto the ground. Our robot was able to perform these tasks fully autonomously.&lt;/p&gt;
&lt;p&gt;My responsibilities: Autonomous Task Control, System Architecture, Task Execution Logging and Profiling, Semantic World Modeling&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SpacebotCamp</title>
      <link>https://sebastian-brunner.github.io/project/spacebotcamp/</link>
      <pubDate>Sun, 13 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://sebastian-brunner.github.io/project/spacebotcamp/</guid>
      <description>&lt;p&gt;In the SpacebotCamp, our mobile robotic system had to explore an unknwon area, find two objects and bring them to a third object for assembly. All tasks had to be done with a high level of autonomy since communication was delayed by 2 seconds and sending commands was limited to only a few occasions. The whole task had to be done in 60 minutes.&lt;/p&gt;
&lt;p&gt;Our team, RMexplores!, was the only team that fulfilled all tasks within the given specification. This was accomplished in half of the time!&lt;/p&gt;
&lt;p&gt;My responsibilities: Autonomous Task Control, System Architecture&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Semantic World Model</title>
      <link>https://sebastian-brunner.github.io/project/world_model/</link>
      <pubDate>Tue, 10 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>https://sebastian-brunner.github.io/project/world_model/</guid>
      <description>&lt;p&gt;The world model is implemented using the graph database Neo4j (&lt;a href=&#34;https://neo4j.com/&#34;&gt;https://neo4j.com/&lt;/a&gt;) as backend.
Neo4j represents data using a property-graph model, meaning the database is a graph composed of nodes and interconnected by edges, both of which can store arbitrary properties in a key-value fashion.
For our world model, we directly represent objects in the world model as nodes in the database graph and correspondingly edges in the graph build up the world model tree structure.
The relative position and orientation between two objects is stored as properties of the connecting edge.
We define and ensure a common type-system for objects in the world, by defining a type-hierarchy with precisely listed required and optional properties for each object type (e.g. every PhysicalObject has to have a mass; every Grasp a width and force).&lt;/p&gt;
&lt;p&gt;A database as backend in general provides useful features to keep the world model consistent at any time, e.g. synchronized multi-client read-/write-access and transaction support for batching multiple modifications into an atomic operation.
In particular, we use transactions with pre-/post-transaction sanity checks to allow complex yet safe operations on the world model. In case of a software error or violated sanity checks (e.g. a world model object suddenly has two parents), the world model is automatically rolled back to the consistent state before this modification.
As a graph database, Neo4j offers a pattern-related query language called CYPHER, which makes it very easy to query the world model for e.g. &amp;ldquo;all rigid bodies for which a grasp is defined.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;We use YAML-files as a human read-/writeable way to specify the initial world model, but also complex object templates (e.g. complete subgraphs of PhysicalObjects together with attached Grasps and Markers, etc.). To add such templates into the world model, we developed a domain-specific module and API for higher level operations based on the raw CYPHER-access to the database.
Other more complex operations include querying the relative transformation between arbitrary objects in the graph or maintaining unique labels for nodes. This API is the basis for several adapter modules which make operations accessible via ROS services. New functionality or composite operations can be added in a modular way by writing additional adapters as necessary.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
